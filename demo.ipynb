{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining on Unlabeled Data\n",
    "\n",
    "- Configuring the global environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerate device:  mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "# Select accelerate device\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Accelerate device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a GPT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture import GPTModel\n",
    "\n",
    "# Hyper-parameters configuration\n",
    "HYPER_PARAMS_CONFIG = {\n",
    "    \"batch_size\" : 2,   \n",
    "    \"num_workers\" : 0,\n",
    "    \"num_epochs\" : 10,\n",
    "    \"lr\" : 4e-4,\n",
    "    \"weight_decay\" : 0.1,\n",
    "}\n",
    "\n",
    "# The gpt2-small (124M) parameter configuration\n",
    "GPT_CONFIG_124M = {\n",
    "        \"vocab_size\" : 50257,    # Vocabulary size\n",
    "        \"context_length\" : 256,  # Shortened context length (orig: 1024)    \n",
    "        \"emb_dim\" : 768,         # Embedding dimension\n",
    "        \"n_heads\" : 12,          # Number of attntion heads\n",
    "        \"n_layers\" : 12,         # Number of layers\n",
    "        \"dropout\" : 0.0,         # Dropout rate\n",
    "        \"qkv_bias\" : False       # Query, Key, and Value bias\n",
    "}    \n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader\n",
    "\n",
    "- Create dataset and dataloader that extract chunks from the input text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(123)\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer: tiktoken.Encoding, sample_length, stride) -> None:\n",
    "        super().__init__()\n",
    "        self.input_idx = []\n",
    "        self.target_idx = []\n",
    "\n",
    "        token_idx = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in  range(0, len(token_idx) - sample_length + 1, stride):\n",
    "            input_chunk = token_idx[i: i + sample_length]\n",
    "            target_chunk = token_idx[i + 1: i + sample_length + 1]\n",
    "            self.input_idx.append(torch.tensor(input_chunk))\n",
    "            self.target_idx.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.target_idx)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_idx[index], self.target_idx[index]\n",
    "    \n",
    "def create_dataloader_V1(txt, batch_size=4, max_length=256, stride=128, \n",
    "                         shuffle=True, drop_last=True, num_workers = 7):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    \n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=shuffle, \n",
    "                            drop_last=drop_last, \n",
    "                            num_workers=num_workers, \n",
    "                )\n",
    "    \n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use a relatively small dataset [the-verdict.txt](./the-verdict.txt) for training the LLM (in fact, only one short story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_V1(\n",
    "    txt=train_data,\n",
    "    batch_size=HYPER_PARAMS_CONFIG[\"batch_size\"],\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=HYPER_PARAMS_CONFIG[\"num_workers\"],\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_V1(\n",
    "    val_data,\n",
    "    batch_size=HYPER_PARAMS_CONFIG['batch_size'],\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=HYPER_PARAMS_CONFIG[\"num_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50256]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The gpt2 encoder of tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=HYPER_PARAMS_CONFIG[\"lr\"], \n",
    "                              weight_decay=HYPER_PARAMS_CONFIG[\"weight_decay\"],\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Trainer\n",
    "\n",
    "Trainer class to encapsulate the training, evaluation, and testing procedures for a PyTorch model.\n",
    "\n",
    "This `Trainer()` supports various features including ***learning rate warmup, cosine decay, gradient clipping, and periodic evaluation***. It can handle both classification and regression tasks.\n",
    "\n",
    "```Python\n",
    "class Trainer():\n",
    "    def __init__(\n",
    "            self, \n",
    "            model: nn.Module, \n",
    "            device: torch.device, \n",
    "            num_epochs: int,\n",
    "            optimizer: torch.optim.Optimizer,\n",
    "            train_loader: DataLoader = None, \n",
    "            valid_loader: DataLoader = None, \n",
    "            eval_freq: int = None, \n",
    "            eval_iter: int = None,\n",
    "            is_classification: bool = False, \n",
    "            warmup: bool = False,\n",
    "            cos_dec: bool = False,\n",
    "            grd_clip: bool = False, \n",
    "            inital_lr: float = 3e-5, \n",
    "            min_lr: float = 1e-6,\n",
    "            checkpoint_path: str = None,\n",
    "            tokenizer: tiktoken.Encoding = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the Trainer object with the provided model, training parameters, and options.\n",
    "\n",
    "        Args:\n",
    "        - model: The PyTorch model.\n",
    "        - device: The device to run the model on ('cpu', 'mps', or 'cuda').\n",
    "        - num_epochs: The number of epochs for training.\n",
    "        - optimizer: The optimizer for training the model.\n",
    "        - train_loader: The DataLoader for training data (default: None).\n",
    "        - valid_loader: The DataLoader for validation data (default: None).\n",
    "        - eval_freq: Evaluation frequency to control how often (in epochs) the model is evaluated \n",
    "                     during training (default: None).\n",
    "        - eval_iter: Evaluation iterations to control the number of batches processed (default: None).\n",
    "        - is_classification: Whether the task is classification, if is_classification is True, only \n",
    "                             the last time step of the model's output is used for loss calculation\n",
    "                             (default: False).\n",
    "        - warmup: Whether to use learning rate warmup (default: False).\n",
    "        - cos_dec: Whether to use cosine decay for learning rate (default: False).\n",
    "        - grd_clip: Whether to use gradient clipping (default: False).\n",
    "        - initial_lr: Initial learning rate with warmup (default: 3e-5).\n",
    "        - min_lr: Minimum learning rate with cosine decay (default: 1e-6).\n",
    "        - checkpoint_path: The path to the directory containing the model checkpoints (default: None).\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren Mortgage TT remember gard ACTIONSussedOND Land Engeledded\n"
     ]
    }
   ],
   "source": [
    "from trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "            model=model, \n",
    "            device=device, \n",
    "            train_loader=train_loader,\n",
    "            valid_loader=val_loader,\n",
    "            is_classification=False,\n",
    "            num_epochs=HYPER_PARAMS_CONFIG[\"num_epochs\"], \n",
    "            optimizer=optimizer,\n",
    "            eval_freq=5,\n",
    "            eval_iter=5,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "gen_text = trainer.text_generator(\"Every effort moves you\", max_new_tokens=20)\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining the GPT Model\n",
    "\n",
    "- Based on the training and validation set losses, we can see that the model starts overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.000, Val loss 10.117\n",
      "Ep 1 (Step 000005): Train loss 8.031, Val loss 8.250\n",
      "Ep 2 (Step 000010): Train loss 6.772, Val loss 7.064\n",
      "Ep 2 (Step 000015): Train loss 5.879, Val loss 6.577\n",
      "Ep 3 (Step 000020): Train loss 5.706, Val loss 6.478\n",
      "Ep 3 (Step 000025): Train loss 5.135, Val loss 6.478\n",
      "Ep 4 (Step 000030): Train loss 4.768, Val loss 6.325\n",
      "Ep 4 (Step 000035): Train loss 4.636, Val loss 6.556\n",
      "Ep 5 (Step 000040): Train loss 3.741, Val loss 6.183\n",
      "Ep 6 (Step 000045): Train loss 3.201, Val loss 6.079\n",
      "Ep 6 (Step 000050): Train loss 2.750, Val loss 6.119\n",
      "Ep 7 (Step 000055): Train loss 2.122, Val loss 6.152\n",
      "Ep 7 (Step 000060): Train loss 1.965, Val loss 6.148\n",
      "Ep 8 (Step 000065): Train loss 1.466, Val loss 6.200\n",
      "Ep 8 (Step 000070): Train loss 1.144, Val loss 6.280\n",
      "Ep 9 (Step 000075): Train loss 0.727, Val loss 6.312\n",
      "Ep 9 (Step 000080): Train loss 0.562, Val loss 6.372\n",
      "Ep 10 (Step 000085): Train loss 0.388, Val loss 6.398\n"
     ]
    }
   ],
   "source": [
    "trainer.training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Again to generate the text after trained, we can see that the model starts out generating incomprehensible strings of words, whereas towards the end, it's able to produce grammatically more or less correct sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "gen_text = trainer.text_generator(\"Every effort moves you\", max_new_tokens=50)\n",
    "print(gen_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can control the distribution and selection process via a concept called temperature scaling. Temperatures greater than 1 will result in more uniformly distributed token probabilities after applying the softmax. Temperatures smaller than 1 will result in more confident distributions after applying the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you in the inevitable up-rooms, in the deep arm one of the Sevres and silver ofident moust slight shade of constraint degree to the display of this false that he's his pictures--the quality of looking cleverer than he had to\n"
     ]
    }
   ],
   "source": [
    "gen_text = trainer.text_generator(\"Every effort moves you\", max_new_tokens=50, temperature=2, top_k=5)\n",
    "print(gen_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
